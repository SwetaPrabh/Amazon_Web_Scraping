{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium\n",
    "#To scrape the unique ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to flatten nested list\n",
    "import itertools\n",
    "\n",
    "def oneDArray(x):\n",
    "    return list(itertools.chain(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review = pd.read_json('reviews_Beauty_5.json',lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1YJEY40YUW4SE</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Andrea</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>Very oily and creamy. Not at all what I expect...</td>\n",
       "      <td>1</td>\n",
       "      <td>Don't waste your money</td>\n",
       "      <td>1391040000</td>\n",
       "      <td>01 30, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A60XNB876KYML</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Jessica H.</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This palette was a decent price and I was look...</td>\n",
       "      <td>3</td>\n",
       "      <td>OK Palette!</td>\n",
       "      <td>1397779200</td>\n",
       "      <td>04 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3G6XNM240RMWA</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Karen</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>The texture of this concealer pallet is fantas...</td>\n",
       "      <td>4</td>\n",
       "      <td>great quality</td>\n",
       "      <td>1378425600</td>\n",
       "      <td>09 6, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1PQFP6SAJ6D80</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Norah</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I really can't tell what exactly this thing is...</td>\n",
       "      <td>2</td>\n",
       "      <td>Do not work on my face</td>\n",
       "      <td>1386460800</td>\n",
       "      <td>12 8, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A38FVHZTNQ271F</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Nova Amor</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>It was a little smaller than I expected, but t...</td>\n",
       "      <td>3</td>\n",
       "      <td>It's okay.</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ASXPNU896V9I8</td>\n",
       "      <td>B00004U9UY</td>\n",
       "      <td>L90</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I have dry hands, as my job requires me to was...</td>\n",
       "      <td>5</td>\n",
       "      <td>Best hand lotion</td>\n",
       "      <td>1360800000</td>\n",
       "      <td>02 14, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A3S3R88HA0HZG3</td>\n",
       "      <td>B00004U9UY</td>\n",
       "      <td>PT Cruiser</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This hand cream has one of the nicest fragranc...</td>\n",
       "      <td>5</td>\n",
       "      <td>An effective hand cream with a wonderful scent</td>\n",
       "      <td>1238544000</td>\n",
       "      <td>04 1, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A2SX3JU39YE2TP</td>\n",
       "      <td>B00004U9UY</td>\n",
       "      <td>Summer Haven</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I first purchased this hand treatment from a g...</td>\n",
       "      <td>4</td>\n",
       "      <td>Wonderful feel, wonderful fragrance</td>\n",
       "      <td>1374192000</td>\n",
       "      <td>07 19, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>AD526SECC3I9D</td>\n",
       "      <td>B00004U9V2</td>\n",
       "      <td>Anuradha J. Cetta</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Best hand cream I've ever used, and I NEVER us...</td>\n",
       "      <td>5</td>\n",
       "      <td>Great cream!</td>\n",
       "      <td>1386201600</td>\n",
       "      <td>12 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ACUGBCEADYT6D</td>\n",
       "      <td>B00004U9V2</td>\n",
       "      <td>booklass \"Passionate bibliophile.\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Love the moisturizing properties of the lotion...</td>\n",
       "      <td>3</td>\n",
       "      <td>I like the moisturizing properties, but the sm...</td>\n",
       "      <td>1402617600</td>\n",
       "      <td>06 13, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin                        reviewerName helpful  \\\n",
       "0   A1YJEY40YUW4SE  7806397051                              Andrea  [3, 4]   \n",
       "1    A60XNB876KYML  7806397051                          Jessica H.  [1, 1]   \n",
       "2   A3G6XNM240RMWA  7806397051                               Karen  [0, 1]   \n",
       "3   A1PQFP6SAJ6D80  7806397051                               Norah  [2, 2]   \n",
       "4   A38FVHZTNQ271F  7806397051                           Nova Amor  [0, 0]   \n",
       "..             ...         ...                                 ...     ...   \n",
       "95   ASXPNU896V9I8  B00004U9UY                                 L90  [0, 0]   \n",
       "96  A3S3R88HA0HZG3  B00004U9UY                          PT Cruiser  [1, 1]   \n",
       "97  A2SX3JU39YE2TP  B00004U9UY                        Summer Haven  [0, 0]   \n",
       "98   AD526SECC3I9D  B00004U9V2                   Anuradha J. Cetta  [0, 0]   \n",
       "99   ACUGBCEADYT6D  B00004U9V2  booklass \"Passionate bibliophile.\"  [0, 0]   \n",
       "\n",
       "                                           reviewText  overall  \\\n",
       "0   Very oily and creamy. Not at all what I expect...        1   \n",
       "1   This palette was a decent price and I was look...        3   \n",
       "2   The texture of this concealer pallet is fantas...        4   \n",
       "3   I really can't tell what exactly this thing is...        2   \n",
       "4   It was a little smaller than I expected, but t...        3   \n",
       "..                                                ...      ...   \n",
       "95  I have dry hands, as my job requires me to was...        5   \n",
       "96  This hand cream has one of the nicest fragranc...        5   \n",
       "97  I first purchased this hand treatment from a g...        4   \n",
       "98  Best hand cream I've ever used, and I NEVER us...        5   \n",
       "99  Love the moisturizing properties of the lotion...        3   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "0                              Don't waste your money      1391040000   \n",
       "1                                         OK Palette!      1397779200   \n",
       "2                                       great quality      1378425600   \n",
       "3                              Do not work on my face      1386460800   \n",
       "4                                          It's okay.      1382140800   \n",
       "..                                                ...             ...   \n",
       "95                                   Best hand lotion      1360800000   \n",
       "96     An effective hand cream with a wonderful scent      1238544000   \n",
       "97                Wonderful feel, wonderful fragrance      1374192000   \n",
       "98                                       Great cream!      1386201600   \n",
       "99  I like the moisturizing properties, but the sm...      1402617600   \n",
       "\n",
       "     reviewTime  \n",
       "0   01 30, 2014  \n",
       "1   04 18, 2014  \n",
       "2    09 6, 2013  \n",
       "3    12 8, 2013  \n",
       "4   10 19, 2013  \n",
       "..          ...  \n",
       "95  02 14, 2013  \n",
       "96   04 1, 2009  \n",
       "97  07 19, 2013  \n",
       "98   12 5, 2013  \n",
       "99  06 13, 2014  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Review.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198502, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of data points\n",
    "Review.shape\n",
    "#There are 198502 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the unique products\n",
    "len(Review['asin'].unique())\n",
    "#There are 12101 unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22363"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the number of unique users\n",
    "len(Review['reviewerID'].unique())\n",
    "#There are 22363 unique user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',\n",
       "       'overall', 'summary', 'unixReviewTime', 'reviewTime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#columns\n",
    "Review.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1YJEY40YUW4SE</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Andrea</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>Very oily and creamy. Not at all what I expect...</td>\n",
       "      <td>1</td>\n",
       "      <td>Don't waste your money</td>\n",
       "      <td>1391040000</td>\n",
       "      <td>01 30, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A60XNB876KYML</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Jessica H.</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This palette was a decent price and I was look...</td>\n",
       "      <td>3</td>\n",
       "      <td>OK Palette!</td>\n",
       "      <td>1397779200</td>\n",
       "      <td>04 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3G6XNM240RMWA</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Karen</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>The texture of this concealer pallet is fantas...</td>\n",
       "      <td>4</td>\n",
       "      <td>great quality</td>\n",
       "      <td>1378425600</td>\n",
       "      <td>09 6, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1PQFP6SAJ6D80</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Norah</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I really can't tell what exactly this thing is...</td>\n",
       "      <td>2</td>\n",
       "      <td>Do not work on my face</td>\n",
       "      <td>1386460800</td>\n",
       "      <td>12 8, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A38FVHZTNQ271F</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>Nova Amor</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>It was a little smaller than I expected, but t...</td>\n",
       "      <td>3</td>\n",
       "      <td>It's okay.</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A3BTN14HIZET6Z</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>S. M. Randall \"WildHorseWoman\"</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>I was very happy to get this palette, now I wi...</td>\n",
       "      <td>5</td>\n",
       "      <td>Very nice palette!</td>\n",
       "      <td>1365984000</td>\n",
       "      <td>04 15, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A1Z59RFKN0M5QL</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>tasha \"luvely12b\"</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>PLEASE DONT DO IT! this just rachett the palet...</td>\n",
       "      <td>1</td>\n",
       "      <td>smh!!!</td>\n",
       "      <td>1376611200</td>\n",
       "      <td>08 16, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AWUO9P6PL1SY8</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>TreMagnifique</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>Chalky,Not Pigmented,Wears off easily,Not a Co...</td>\n",
       "      <td>2</td>\n",
       "      <td>Chalky, Not Pigmented, Wears off easily, Not a...</td>\n",
       "      <td>1378252800</td>\n",
       "      <td>09 4, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A3LMILRM9OC3SA</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Did nothing for me. Stings when I put it on. I...</td>\n",
       "      <td>2</td>\n",
       "      <td>no Lightening, no Brightening,......NOTHING</td>\n",
       "      <td>1405209600</td>\n",
       "      <td>07 13, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A30IP88QK3YUIO</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>Amina Bint Ibraheem</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought this product to get rid of the dark s...</td>\n",
       "      <td>3</td>\n",
       "      <td>Its alright</td>\n",
       "      <td>1388102400</td>\n",
       "      <td>12 27, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>APBQH4BS48CQO</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>Charmmy</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I have mixed feelings about this product. When...</td>\n",
       "      <td>3</td>\n",
       "      <td>Mixed feelings.</td>\n",
       "      <td>1400544000</td>\n",
       "      <td>05 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A3FE8W8UV95U6B</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>Culture C Simmons</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Did nothing for my skin. Used as suggested and...</td>\n",
       "      <td>1</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>1392681600</td>\n",
       "      <td>02 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A1EVGDOTGFZOSS</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>Jessica \"Anarchykisses\"</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I bought this product about 3 months ago, I fi...</td>\n",
       "      <td>5</td>\n",
       "      <td>This works</td>\n",
       "      <td>1390435200</td>\n",
       "      <td>01 23, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AP5WTCMP6DTRV</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>Layla B</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This gell did nothing at all. I religiously pu...</td>\n",
       "      <td>1</td>\n",
       "      <td>Does nothing</td>\n",
       "      <td>1389398400</td>\n",
       "      <td>01 11, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A21IM16PQWKVO5</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>mdub9922</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>i got this to get rid of a scar and it did jus...</td>\n",
       "      <td>5</td>\n",
       "      <td>it works</td>\n",
       "      <td>1392681600</td>\n",
       "      <td>02 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A1TLDR1V4O48PK</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>Mickey O Neil \"Mickey O Neil\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I used it for anal bleaching and it burned a b...</td>\n",
       "      <td>2</td>\n",
       "      <td>burns</td>\n",
       "      <td>1396742400</td>\n",
       "      <td>04 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A6F8KH0J1AVYA</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>SanBen</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>I order this cream along with their soap. It a...</td>\n",
       "      <td>5</td>\n",
       "      <td>Did work for me</td>\n",
       "      <td>1379116800</td>\n",
       "      <td>09 14, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AXPKZA7UZXKTT</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>Shirleyyy</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>Good product. Use a little bit on your spot an...</td>\n",
       "      <td>4</td>\n",
       "      <td>excellent</td>\n",
       "      <td>1382054400</td>\n",
       "      <td>10 18, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A2SIAYDK7GG7QA</td>\n",
       "      <td>9759091062</td>\n",
       "      <td>theredtranny</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I didn't use it past a week. The reason why is...</td>\n",
       "      <td>3</td>\n",
       "      <td>weird smell</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>11 1, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A1QV5IH6HDRN0L</td>\n",
       "      <td>9788072216</td>\n",
       "      <td>armygirl</td>\n",
       "      <td>[24, 24]</td>\n",
       "      <td>I haven't been a big fan of Prada's fragrances...</td>\n",
       "      <td>5</td>\n",
       "      <td>Love the smell of this!</td>\n",
       "      <td>1316390400</td>\n",
       "      <td>09 19, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A3UQXHI88S7XAX</td>\n",
       "      <td>9788072216</td>\n",
       "      <td>D. Greene</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We gave these as gifts and everyone that recei...</td>\n",
       "      <td>5</td>\n",
       "      <td>Happy</td>\n",
       "      <td>1376092800</td>\n",
       "      <td>08 10, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A2EK2CJNJUF7OQ</td>\n",
       "      <td>9788072216</td>\n",
       "      <td>Nikki</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This is the first fragrance by Prada that I lo...</td>\n",
       "      <td>5</td>\n",
       "      <td>Very good</td>\n",
       "      <td>1322438400</td>\n",
       "      <td>11 28, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A2GWNGQF9SHRE4</td>\n",
       "      <td>9788072216</td>\n",
       "      <td>Pholuke \"Lepa Shandy\"</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>So I got this about a month ago. I had no plan...</td>\n",
       "      <td>5</td>\n",
       "      <td>Lurrrrrrrrv.....</td>\n",
       "      <td>1338076800</td>\n",
       "      <td>05 27, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ABV67T136UXFQ</td>\n",
       "      <td>9788072216</td>\n",
       "      <td>Sandra</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This product has a very fruity scent which is ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Scent</td>\n",
       "      <td>1359763200</td>\n",
       "      <td>02 2, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A2FQZKL2KIZACO</td>\n",
       "      <td>9790790961</td>\n",
       "      <td>Ellie B.</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>I'm very picky when it comes to fragrance. I l...</td>\n",
       "      <td>5</td>\n",
       "      <td>Spring Garden in a Bottle</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>03 11, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A312RDWQYLAG7S</td>\n",
       "      <td>9790790961</td>\n",
       "      <td>ltg</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>bright crystals reminds me of Victoria Secrets...</td>\n",
       "      <td>3</td>\n",
       "      <td>fresh smell</td>\n",
       "      <td>1324252800</td>\n",
       "      <td>12 19, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A3TYR1ALBZ2EU9</td>\n",
       "      <td>9790790961</td>\n",
       "      <td>Mananagirl6</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Got this product and I never heard of this so ...</td>\n",
       "      <td>5</td>\n",
       "      <td>My new smell!</td>\n",
       "      <td>1378598400</td>\n",
       "      <td>09 8, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AUYVMLI0CBMYS</td>\n",
       "      <td>9790790961</td>\n",
       "      <td>Shay1234</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>This is a beautiful perfume! Nice, clean scent...</td>\n",
       "      <td>5</td>\n",
       "      <td>One of my favs!</td>\n",
       "      <td>1391299200</td>\n",
       "      <td>02 2, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A2H0VDRANZMGGX</td>\n",
       "      <td>9790790961</td>\n",
       "      <td>Shoe luva</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>This is the real Versace Bright Crystal fragra...</td>\n",
       "      <td>5</td>\n",
       "      <td>Smells clean</td>\n",
       "      <td>1392336000</td>\n",
       "      <td>02 14, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>A2MZDI3I5AEL74</td>\n",
       "      <td>9790790961</td>\n",
       "      <td>Y. Siani \"I love nini\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I love it! and will continue using it but it's...</td>\n",
       "      <td>3</td>\n",
       "      <td>Love the smell but a shame it's not last as long</td>\n",
       "      <td>1392163200</td>\n",
       "      <td>02 12, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>A17PVB3HRDWN2B</td>\n",
       "      <td>9790794231</td>\n",
       "      <td>hippie chick \"hippie chick\"</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I like this perfume. I first discovered it whe...</td>\n",
       "      <td>3</td>\n",
       "      <td>yummy soft smell</td>\n",
       "      <td>1255392000</td>\n",
       "      <td>10 13, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>A3R9H6OKZHHRJD</td>\n",
       "      <td>9790794231</td>\n",
       "      <td>LH422</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a very unique scent! It's deep and mys...</td>\n",
       "      <td>4</td>\n",
       "      <td>intriguing</td>\n",
       "      <td>1402185600</td>\n",
       "      <td>06 8, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A2II54B3VA45LN</td>\n",
       "      <td>9790794231</td>\n",
       "      <td>Professional shopper</td>\n",
       "      <td>[1, 15]</td>\n",
       "      <td>I am not a perfume wearer, but I heard that th...</td>\n",
       "      <td>2</td>\n",
       "      <td>Not for me</td>\n",
       "      <td>1207008000</td>\n",
       "      <td>04 1, 2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A2W0O92FZFHJOJ</td>\n",
       "      <td>9790794231</td>\n",
       "      <td>Ronnie G.</td>\n",
       "      <td>[2, 8]</td>\n",
       "      <td>Sorry about the title. . . I couldn't help mys...</td>\n",
       "      <td>4</td>\n",
       "      <td>STELLLAAA!!!!!</td>\n",
       "      <td>1234310400</td>\n",
       "      <td>02 11, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ARXL1MF07FKY3</td>\n",
       "      <td>9790794231</td>\n",
       "      <td>Yvonne Trevino</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Love the way this parfum smells.  It is the be...</td>\n",
       "      <td>5</td>\n",
       "      <td>SMELLS WONDERFUL</td>\n",
       "      <td>1392163200</td>\n",
       "      <td>02 12, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>AVM6OR01CPJNQ</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>AM</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>We were pretty disappointed with this shampoo ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Not impressed</td>\n",
       "      <td>1365724800</td>\n",
       "      <td>04 12, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>A1ISPXMZL1IYSE</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Bernadette Foster</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I washed and conditioned 1 a week but didn't s...</td>\n",
       "      <td>2</td>\n",
       "      <td>Didn't see results</td>\n",
       "      <td>1358726400</td>\n",
       "      <td>01 21, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>A16NAH39LN211W</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Blondie</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>My hair is fine like a baby's and rather thin....</td>\n",
       "      <td>5</td>\n",
       "      <td>Really good stuff</td>\n",
       "      <td>1395446400</td>\n",
       "      <td>03 22, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>A34M18U6T33YDZ</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Brenda J. Coffin</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Another great product.  Just love this Biotin ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Biotin B-complex thickening conditioner</td>\n",
       "      <td>1396828800</td>\n",
       "      <td>04 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>A8TWSBLUYJR0U</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>carra</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>best conditioner ever- better than most shampo...</td>\n",
       "      <td>5</td>\n",
       "      <td>LOVE THIS BRAND!~</td>\n",
       "      <td>1385164800</td>\n",
       "      <td>11 23, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>A149N75TZ34CE5</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Chi Le \"CL\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Please avoid this product. The conditional is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Bad product</td>\n",
       "      <td>1373760000</td>\n",
       "      <td>07 14, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>AUJZEJ5KJU6OQ</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Debra Carr \"Peace and Good Health\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I have thin fine hair and I am always searchin...</td>\n",
       "      <td>1</td>\n",
       "      <td>GULLIBLE</td>\n",
       "      <td>1344729600</td>\n",
       "      <td>08 12, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>A2EVBQAN92IEZS</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Fati1981</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>Rather than leaving my hair softer and more ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>Doesn't live up to its promises</td>\n",
       "      <td>1379030400</td>\n",
       "      <td>09 13, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>A2VOGNBUMXSW13</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Hon Lee \"Bao Tao\"</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I BOUGHT THIS Avalon Organics: Biotin B Comple...</td>\n",
       "      <td>2</td>\n",
       "      <td>DRIED MY HAIR</td>\n",
       "      <td>1357516800</td>\n",
       "      <td>01 7, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>A2SPAROS5PSI8X</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>isthisvick</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>So i have been using the Avalon Organics both ...</td>\n",
       "      <td>4</td>\n",
       "      <td>5 months after.. Good improvement!</td>\n",
       "      <td>1337904000</td>\n",
       "      <td>05 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>A22PKZZK5DSONS</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Jeremy C</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I have changed an earlier review on this produ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Gives the appearance of thicker hair.</td>\n",
       "      <td>1356393600</td>\n",
       "      <td>12 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>AX1567KGMAVLT</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>jjjjordan</td>\n",
       "      <td>[7, 8]</td>\n",
       "      <td>With the inevitable hormonal changes, my hair ...</td>\n",
       "      <td>5</td>\n",
       "      <td>very surprised how much I love this</td>\n",
       "      <td>1300060800</td>\n",
       "      <td>03 14, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>AQX8T9SFFK43U</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Kristina Montes</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>6/8/2013, first review: I thought to review th...</td>\n",
       "      <td>4</td>\n",
       "      <td>Losing less hair!  See detailed review.</td>\n",
       "      <td>1370649600</td>\n",
       "      <td>06 8, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>A2ZQE4X9GYJMRI</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>Meno</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought this along with the shampoo because m...</td>\n",
       "      <td>5</td>\n",
       "      <td>Helped with hair loss</td>\n",
       "      <td>1324339200</td>\n",
       "      <td>12 20, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>A38VDDNZ7A3QM0</td>\n",
       "      <td>B00004TMFE</td>\n",
       "      <td>rey</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I love it. Will buy again! although it is a bi...</td>\n",
       "      <td>5</td>\n",
       "      <td>will purchase again</td>\n",
       "      <td>1386028800</td>\n",
       "      <td>12 3, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin                        reviewerName   helpful  \\\n",
       "0   A1YJEY40YUW4SE  7806397051                              Andrea    [3, 4]   \n",
       "1    A60XNB876KYML  7806397051                          Jessica H.    [1, 1]   \n",
       "2   A3G6XNM240RMWA  7806397051                               Karen    [0, 1]   \n",
       "3   A1PQFP6SAJ6D80  7806397051                               Norah    [2, 2]   \n",
       "4   A38FVHZTNQ271F  7806397051                           Nova Amor    [0, 0]   \n",
       "5   A3BTN14HIZET6Z  7806397051      S. M. Randall \"WildHorseWoman\"    [1, 2]   \n",
       "6   A1Z59RFKN0M5QL  7806397051                   tasha \"luvely12b\"    [1, 3]   \n",
       "7    AWUO9P6PL1SY8  7806397051                       TreMagnifique    [0, 1]   \n",
       "8   A3LMILRM9OC3SA  9759091062                                 NaN    [0, 0]   \n",
       "9   A30IP88QK3YUIO  9759091062                 Amina Bint Ibraheem    [0, 0]   \n",
       "10   APBQH4BS48CQO  9759091062                             Charmmy    [0, 0]   \n",
       "11  A3FE8W8UV95U6B  9759091062                   Culture C Simmons    [0, 0]   \n",
       "12  A1EVGDOTGFZOSS  9759091062             Jessica \"Anarchykisses\"    [0, 1]   \n",
       "13   AP5WTCMP6DTRV  9759091062                             Layla B    [0, 0]   \n",
       "14  A21IM16PQWKVO5  9759091062                            mdub9922    [0, 1]   \n",
       "15  A1TLDR1V4O48PK  9759091062       Mickey O Neil \"Mickey O Neil\"    [0, 0]   \n",
       "16   A6F8KH0J1AVYA  9759091062                              SanBen    [2, 4]   \n",
       "17   AXPKZA7UZXKTT  9759091062                           Shirleyyy    [2, 4]   \n",
       "18  A2SIAYDK7GG7QA  9759091062                        theredtranny    [0, 1]   \n",
       "19  A1QV5IH6HDRN0L  9788072216                            armygirl  [24, 24]   \n",
       "20  A3UQXHI88S7XAX  9788072216                           D. Greene    [0, 0]   \n",
       "21  A2EK2CJNJUF7OQ  9788072216                               Nikki    [1, 1]   \n",
       "22  A2GWNGQF9SHRE4  9788072216               Pholuke \"Lepa Shandy\"    [2, 4]   \n",
       "23   ABV67T136UXFQ  9788072216                              Sandra    [0, 0]   \n",
       "24  A2FQZKL2KIZACO  9790790961                            Ellie B.    [1, 1]   \n",
       "25  A312RDWQYLAG7S  9790790961                                 ltg    [0, 1]   \n",
       "26  A3TYR1ALBZ2EU9  9790790961                         Mananagirl6    [0, 0]   \n",
       "27   AUYVMLI0CBMYS  9790790961                            Shay1234    [0, 1]   \n",
       "28  A2H0VDRANZMGGX  9790790961                           Shoe luva    [1, 2]   \n",
       "29  A2MZDI3I5AEL74  9790790961              Y. Siani \"I love nini\"    [0, 0]   \n",
       "30  A17PVB3HRDWN2B  9790794231         hippie chick \"hippie chick\"    [0, 1]   \n",
       "31  A3R9H6OKZHHRJD  9790794231                               LH422    [0, 0]   \n",
       "32  A2II54B3VA45LN  9790794231                Professional shopper   [1, 15]   \n",
       "33  A2W0O92FZFHJOJ  9790794231                           Ronnie G.    [2, 8]   \n",
       "34   ARXL1MF07FKY3  9790794231                      Yvonne Trevino    [0, 0]   \n",
       "35   AVM6OR01CPJNQ  B00004TMFE                                  AM    [0, 1]   \n",
       "36  A1ISPXMZL1IYSE  B00004TMFE                   Bernadette Foster    [0, 1]   \n",
       "37  A16NAH39LN211W  B00004TMFE                             Blondie    [2, 3]   \n",
       "38  A34M18U6T33YDZ  B00004TMFE                    Brenda J. Coffin    [0, 0]   \n",
       "39   A8TWSBLUYJR0U  B00004TMFE                               carra    [0, 0]   \n",
       "40  A149N75TZ34CE5  B00004TMFE                         Chi Le \"CL\"    [0, 0]   \n",
       "41   AUJZEJ5KJU6OQ  B00004TMFE  Debra Carr \"Peace and Good Health\"    [0, 0]   \n",
       "42  A2EVBQAN92IEZS  B00004TMFE                            Fati1981    [3, 3]   \n",
       "43  A2VOGNBUMXSW13  B00004TMFE                   Hon Lee \"Bao Tao\"    [0, 1]   \n",
       "44  A2SPAROS5PSI8X  B00004TMFE                          isthisvick    [0, 0]   \n",
       "45  A22PKZZK5DSONS  B00004TMFE                            Jeremy C    [0, 0]   \n",
       "46   AX1567KGMAVLT  B00004TMFE                           jjjjordan    [7, 8]   \n",
       "47   AQX8T9SFFK43U  B00004TMFE                     Kristina Montes    [3, 4]   \n",
       "48  A2ZQE4X9GYJMRI  B00004TMFE                                Meno    [0, 0]   \n",
       "49  A38VDDNZ7A3QM0  B00004TMFE                                 rey    [0, 0]   \n",
       "\n",
       "                                           reviewText  overall  \\\n",
       "0   Very oily and creamy. Not at all what I expect...        1   \n",
       "1   This palette was a decent price and I was look...        3   \n",
       "2   The texture of this concealer pallet is fantas...        4   \n",
       "3   I really can't tell what exactly this thing is...        2   \n",
       "4   It was a little smaller than I expected, but t...        3   \n",
       "5   I was very happy to get this palette, now I wi...        5   \n",
       "6   PLEASE DONT DO IT! this just rachett the palet...        1   \n",
       "7   Chalky,Not Pigmented,Wears off easily,Not a Co...        2   \n",
       "8   Did nothing for me. Stings when I put it on. I...        2   \n",
       "9   I bought this product to get rid of the dark s...        3   \n",
       "10  I have mixed feelings about this product. When...        3   \n",
       "11  Did nothing for my skin. Used as suggested and...        1   \n",
       "12  I bought this product about 3 months ago, I fi...        5   \n",
       "13  This gell did nothing at all. I religiously pu...        1   \n",
       "14  i got this to get rid of a scar and it did jus...        5   \n",
       "15  I used it for anal bleaching and it burned a b...        2   \n",
       "16  I order this cream along with their soap. It a...        5   \n",
       "17  Good product. Use a little bit on your spot an...        4   \n",
       "18  I didn't use it past a week. The reason why is...        3   \n",
       "19  I haven't been a big fan of Prada's fragrances...        5   \n",
       "20  We gave these as gifts and everyone that recei...        5   \n",
       "21  This is the first fragrance by Prada that I lo...        5   \n",
       "22  So I got this about a month ago. I had no plan...        5   \n",
       "23  This product has a very fruity scent which is ...        5   \n",
       "24  I'm very picky when it comes to fragrance. I l...        5   \n",
       "25  bright crystals reminds me of Victoria Secrets...        3   \n",
       "26  Got this product and I never heard of this so ...        5   \n",
       "27  This is a beautiful perfume! Nice, clean scent...        5   \n",
       "28  This is the real Versace Bright Crystal fragra...        5   \n",
       "29  I love it! and will continue using it but it's...        3   \n",
       "30  I like this perfume. I first discovered it whe...        3   \n",
       "31  This is a very unique scent! It's deep and mys...        4   \n",
       "32  I am not a perfume wearer, but I heard that th...        2   \n",
       "33  Sorry about the title. . . I couldn't help mys...        4   \n",
       "34  Love the way this parfum smells.  It is the be...        5   \n",
       "35  We were pretty disappointed with this shampoo ...        2   \n",
       "36  I washed and conditioned 1 a week but didn't s...        2   \n",
       "37  My hair is fine like a baby's and rather thin....        5   \n",
       "38  Another great product.  Just love this Biotin ...        5   \n",
       "39  best conditioner ever- better than most shampo...        5   \n",
       "40  Please avoid this product. The conditional is ...        1   \n",
       "41  I have thin fine hair and I am always searchin...        1   \n",
       "42  Rather than leaving my hair softer and more ma...        1   \n",
       "43  I BOUGHT THIS Avalon Organics: Biotin B Comple...        2   \n",
       "44  So i have been using the Avalon Organics both ...        4   \n",
       "45  I have changed an earlier review on this produ...        4   \n",
       "46  With the inevitable hormonal changes, my hair ...        5   \n",
       "47  6/8/2013, first review: I thought to review th...        4   \n",
       "48  I bought this along with the shampoo because m...        5   \n",
       "49  I love it. Will buy again! although it is a bi...        5   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "0                              Don't waste your money      1391040000   \n",
       "1                                         OK Palette!      1397779200   \n",
       "2                                       great quality      1378425600   \n",
       "3                              Do not work on my face      1386460800   \n",
       "4                                          It's okay.      1382140800   \n",
       "5                                  Very nice palette!      1365984000   \n",
       "6                                              smh!!!      1376611200   \n",
       "7   Chalky, Not Pigmented, Wears off easily, Not a...      1378252800   \n",
       "8         no Lightening, no Brightening,......NOTHING      1405209600   \n",
       "9                                         Its alright      1388102400   \n",
       "10                                    Mixed feelings.      1400544000   \n",
       "11                                            Nothing      1392681600   \n",
       "12                                         This works      1390435200   \n",
       "13                                       Does nothing      1389398400   \n",
       "14                                           it works      1392681600   \n",
       "15                                              burns      1396742400   \n",
       "16                                    Did work for me      1379116800   \n",
       "17                                          excellent      1382054400   \n",
       "18                                        weird smell      1383264000   \n",
       "19                            Love the smell of this!      1316390400   \n",
       "20                                              Happy      1376092800   \n",
       "21                                          Very good      1322438400   \n",
       "22                                   Lurrrrrrrrv.....      1338076800   \n",
       "23                                        Great Scent      1359763200   \n",
       "24                          Spring Garden in a Bottle      1394496000   \n",
       "25                                        fresh smell      1324252800   \n",
       "26                                      My new smell!      1378598400   \n",
       "27                                    One of my favs!      1391299200   \n",
       "28                                       Smells clean      1392336000   \n",
       "29   Love the smell but a shame it's not last as long      1392163200   \n",
       "30                                   yummy soft smell      1255392000   \n",
       "31                                         intriguing      1402185600   \n",
       "32                                         Not for me      1207008000   \n",
       "33                                     STELLLAAA!!!!!      1234310400   \n",
       "34                                   SMELLS WONDERFUL      1392163200   \n",
       "35                                      Not impressed      1365724800   \n",
       "36                                 Didn't see results      1358726400   \n",
       "37                                  Really good stuff      1395446400   \n",
       "38            Biotin B-complex thickening conditioner      1396828800   \n",
       "39                                  LOVE THIS BRAND!~      1385164800   \n",
       "40                                        Bad product      1373760000   \n",
       "41                                           GULLIBLE      1344729600   \n",
       "42                    Doesn't live up to its promises      1379030400   \n",
       "43                                      DRIED MY HAIR      1357516800   \n",
       "44                 5 months after.. Good improvement!      1337904000   \n",
       "45              Gives the appearance of thicker hair.      1356393600   \n",
       "46                very surprised how much I love this      1300060800   \n",
       "47            Losing less hair!  See detailed review.      1370649600   \n",
       "48                              Helped with hair loss      1324339200   \n",
       "49                                will purchase again      1386028800   \n",
       "\n",
       "     reviewTime  \n",
       "0   01 30, 2014  \n",
       "1   04 18, 2014  \n",
       "2    09 6, 2013  \n",
       "3    12 8, 2013  \n",
       "4   10 19, 2013  \n",
       "5   04 15, 2013  \n",
       "6   08 16, 2013  \n",
       "7    09 4, 2013  \n",
       "8   07 13, 2014  \n",
       "9   12 27, 2013  \n",
       "10  05 20, 2014  \n",
       "11  02 18, 2014  \n",
       "12  01 23, 2014  \n",
       "13  01 11, 2014  \n",
       "14  02 18, 2014  \n",
       "15   04 6, 2014  \n",
       "16  09 14, 2013  \n",
       "17  10 18, 2013  \n",
       "18   11 1, 2013  \n",
       "19  09 19, 2011  \n",
       "20  08 10, 2013  \n",
       "21  11 28, 2011  \n",
       "22  05 27, 2012  \n",
       "23   02 2, 2013  \n",
       "24  03 11, 2014  \n",
       "25  12 19, 2011  \n",
       "26   09 8, 2013  \n",
       "27   02 2, 2014  \n",
       "28  02 14, 2014  \n",
       "29  02 12, 2014  \n",
       "30  10 13, 2009  \n",
       "31   06 8, 2014  \n",
       "32   04 1, 2008  \n",
       "33  02 11, 2009  \n",
       "34  02 12, 2014  \n",
       "35  04 12, 2013  \n",
       "36  01 21, 2013  \n",
       "37  03 22, 2014  \n",
       "38   04 7, 2014  \n",
       "39  11 23, 2013  \n",
       "40  07 14, 2013  \n",
       "41  08 12, 2012  \n",
       "42  09 13, 2013  \n",
       "43   01 7, 2013  \n",
       "44  05 25, 2012  \n",
       "45  12 25, 2012  \n",
       "46  03 14, 2011  \n",
       "47   06 8, 2013  \n",
       "48  12 20, 2011  \n",
       "49   12 3, 2013  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Review.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "1. reviewerID: This column gives the unique identity to each amazon user.\n",
    "2 asin: This column gives the product number\n",
    "3. helpful: List L, where L[0]: Number of people who found it helpful, L[1]: Number of people who did not find it helpful\n",
    "4.reviewText: Complete descriotion of the review\n",
    "5.overall: overall rating of the product\n",
    "6.Summary: brief into to the review\n",
    "7.unixReviewTime: Time of review\n",
    "8. reviewTime: Date of Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asin column will be used to scrape additional data from the current amazon product website\n",
    "A list containing unique asin id is formed which will be used to iterate during web scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_asin =[]\n",
    "unique_asin = list(Review['asin'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unique_asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7806397051',\n",
       " '9759091062',\n",
       " '9788072216',\n",
       " '9790790961',\n",
       " '9790794231',\n",
       " 'B00004TMFE',\n",
       " 'B00004TUBL',\n",
       " 'B00004TUBV',\n",
       " 'B00004U9UY',\n",
       " 'B00004U9V2']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_asin[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique_asin list will be iterated through to enter the product page and scrape for more information. Due to block from anti robot pages, fields with unavailable rows will be rescraped using a brand new driver object and then visiting the same url again. \n",
    "The url of the product page is of the format: https://www.amazon.com/gp/product/{asin id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # #Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The file did not have updated product name, color and description, we will scrape it from amazon website\n",
    "import selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the web driver\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an empty list to store dictionary for each product\n",
    "list_of_products = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique_asin list is first iterated and tags are located to scrape the following data\n",
    "1. Product name\n",
    "2. Price\n",
    "3. Rating\n",
    "4. Number of Rating\n",
    "5. Category of beauty product\n",
    "6. Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'No_of_Rating': '5,646 ratings',\n",
      "    'asin': 'B00004TMFE',\n",
      "    'category': 'Hair Care',\n",
      "    'description': 'Brand Avalon Organics Scent Biotin B Hair Type Thin Liquid '\n",
      "                   'Volume 14 Fluid Ounces Ingredients Aloe Barbadensis Leaf '\n",
      "                   'Juice (Certified Organic Ingredient), Aqua (Water), '\n",
      "                   'Brassica Alcohol, Glycerin, Cocos Nucifera (Coconut) Oil '\n",
      "                   '(Certified Organic Ingredient), Brassicyl Isoleucinate '\n",
      "                   'Esylate, Gluconolactone, Capryloyl Glycerin/Sebacic Acid '\n",
      "                   'Copolymer, Cedrus Atlantica Wood Oil, Citrus Aurantifolia '\n",
      "                   '(Lime) Oil, Citrus Aurantium Dulcis (Orange) Peel Oil, '\n",
      "                   'Citrus Limon (Lemon) Peel Oil, Citrus Paradisi '\n",
      "                   '(Grapefruit) Peel Oil, Eucalyptus Globulus Leaf Oil, '\n",
      "                   'Juniperus Virginiana Oil, Persea Gratissima (Avocado) Oil '\n",
      "                   '(Certified Organic Ingredient), Rosmarinus Officinalis '\n",
      "                   '(Rosemary) Leaf Oil, Zingiber Officinale (Ginger) Root '\n",
      "                   'Oil, Avena Sativa (Oat) Kernel Extract (Certified Organic '\n",
      "                   'Ingredient) (Known as Biotin Carrier), Calendula '\n",
      "                   'Officinalis Flower Extract (Certified Organic Ingredient), '\n",
      "                   'Chamomilla Recutita (Matricaria) Flower Extract (Certified '\n",
      "                   'Organic Ingredient), Chenopodium Quinoa Seed (Certified '\n",
      "                   'Organic Ingredient), Citrus Aurantium Dulcis (Orange) '\n",
      "                   'Fruit Extract, Citrus Grandis (Grapefruit) Fruit Extract, '\n",
      "                   'Daucus Carota Sativa (Carrot) Root Extract (Certified '\n",
      "                   'Organic Ingredient), Lavandula Angustifolia (Lavender) '\n",
      "                   'Flower/Leaf/Stem Extract (Certified Organic Ingredient), '\n",
      "                   'Persea Gratissima (Avocado) Fruit Extract (Certified '\n",
      "                   'Organic Ingredient) (Known as Biotin Carrier), Rubus '\n",
      "                   'Idaeus (Raspberry) Fruit Extract (Certified Organic '\n",
      "                   'Ingredient) (Known as Biotin Carrier), Serenoa Serrulata '\n",
      "                   'Fruit Extract (Certified Organic Ingredient) (Saw Palmetto '\n",
      "                   'Extract), Solanum Lycopersicum (Tomato) Fruit/Leaf/Stem '\n",
      "                   'Extract (Certified Organic Ingredient) (Known as Biotin '\n",
      "                   'Carrier), Panthenol, Tocopheryl Acetate, Arachidyl '\n",
      "                   'Alcohol, Arginine, Behenyl Alcohol, Beta-Caryophyllene, '\n",
      "                   'Diheptyl Succinate, Glyceryl Stearate, Stearyl Alcohol '\n",
      "                   '(Certified Organic Ingredient), Calcium Gluconate, Sodium '\n",
      "                   'Benzoate, Citral, Limonene.Aloe Barbadensis Leaf Juice '\n",
      "                   '(Certified Organic Ingredient), Aqua (Water), Brassica '\n",
      "                   'Alcohol, Glycerin, Cocos Nucifera (Coconut) Oil (Certified '\n",
      "                   'Organic Ingreâ€¦ See more',\n",
      "    'name': 'Avalon Organics Therapy Thickening Conditioner, Biotin B-Complex, '\n",
      "            '14 Oz',\n",
      "    'price': '$7.81',\n",
      "    'rating': '4.1 out of 5 stars'}\n"
     ]
    }
   ],
   "source": [
    "#Instantiating the web driver for product with asin B00004TMFE\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "driver.get(f'https://www.amazon.com/gp/product/B00004TMFE')\n",
    "\n",
    "# Instantiate BeautifulSoup web scraper \n",
    "soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "\n",
    "#dictionary to store the key-value pair\n",
    "prod1_dict = {}\n",
    "\n",
    "# Save product ID \n",
    "prod1_dict['asin'] = 'B00004TMFE'\n",
    "\n",
    "# Look for product title \n",
    "prod1_dict['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "# Look for product categories list \n",
    "prod1_dict['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "        \n",
    "# Look for product description\n",
    "desc = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "prod1_dict['description'] =\" \".join(desc.split())\n",
    "        \n",
    "#Look for Product Price\n",
    "prod1_dict['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "#Look for Rating\n",
    "prod1_dict['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "#Number of Ratings\n",
    "prod1_dict['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "\n",
    "pp.pprint(prod1_dict)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping First 10 Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_products_10 = []\n",
    "list_of_unavailable_products_10 =[]\n",
    "\n",
    "#Instantiating the web driver(product 1-10)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "\n",
    "#Start looping through first 500 products in the asin list\n",
    "for i in unique_asin[:10]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "        \n",
    "        # Look for product description\n",
    "        desc = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        product_info['description']=\" \".join(desc.split())\n",
    "        \n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_10.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_10.append(i)\n",
    "        print('not available',i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7806397051',\n",
       " '9759091062',\n",
       " '9790790961',\n",
       " '9790794231',\n",
       " 'B00004U9UY',\n",
       " 'B00004U9V2']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_unavailable_products_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_products_10))\n",
    "print(len(list_of_unavailable_products_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_products_10 = pd.DataFrame(list_of_products_10)\n",
    "\n",
    "list_of_products_10.to_csv('list_of_products_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_10 = pd.DataFrame(list_of_unavailable_products_10)\n",
    "\n",
    "list_of_unavailable_products_10.to_csv('list_of_unavailable_products_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_unavailable_products_10=list_of_unavailable_products_10.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['7806397051'],\n",
       " ['9759091062'],\n",
       " ['9790790961'],\n",
       " ['9790794231'],\n",
       " ['B00004U9UY'],\n",
       " ['B00004U9V2']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_unavailable_products_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7806397051',\n",
       " '9759091062',\n",
       " '9790790961',\n",
       " '9790794231',\n",
       " 'B00004U9UY',\n",
       " 'B00004U9V2']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneDArray(list_of_unavailable_products_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat 'list_of_products_10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         asin                                               name   category  \\\n",
      "0  9788072216  Prada Candy by Prada for Women 1.7 oz Eau de P...  Fragrance   \n",
      "1  B00004TMFE  Avalon Organics Therapy Thickening Conditioner...  Hair Care   \n",
      "2  B00004TUBL  CLASSIC Better Living Two Chamber Dispenser, W...       Bath   \n",
      "3  B00004TUBV  Better Living Products 73350 ULTI-MATE 3 Chamb...       Bath   \n",
      "\n",
      "                                         description   price  \\\n",
      "0  Brand Prada Scent Honey, Musk , Vanilla Item F...  $56.29   \n",
      "1  Brand Avalon Organics Scent Biotin B Hair Type...   $7.81   \n",
      "2  Color White Brand CLASSIC Item Dimensions LxWx...  $39.99   \n",
      "3  Color White Material Plastic Brand Better Livi...  $38.21   \n",
      "\n",
      "               rating   No_of_Rating  \n",
      "0  4.7 out of 5 stars  1,391 ratings  \n",
      "1  4.1 out of 5 stars  5,646 ratings  \n",
      "2  4.3 out of 5 stars     99 ratings  \n",
      "3  4.5 out of 5 stars  1,245 ratings  \n"
     ]
    }
   ],
   "source": [
    "pp.pprint(list_of_products_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Scrape Products 1-2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_products_2000 = []\n",
    "list_of_unavailable_products_2000 =[]\n",
    "\n",
    "#Instantiating the web driver(product 1-500)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "\n",
    "#Start looping through first 500 products in the asin list\n",
    "for i in unique_asin[:500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "       \n",
    "        # Look for product description\n",
    "        desc = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        product_info['description']=\" \".join(desc.split())\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_2000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_2000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 500-1000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "\n",
    "\n",
    "#Start looping through 500-1000 products in the asin list\n",
    "for i in unique_asin[500:1000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_2000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_2000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 1000-15000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 1000-1500 products in the asin list\n",
    "for i in unique_asin[1000:1500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_2000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_2000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 1500-2000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 1000-1500 products in the asin list\n",
    "for i in unique_asin[1500:2000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_2000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_2000.append(i)\n",
    "        print('not available',i)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846\n",
      "1154\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_products_2000))\n",
    "print(len(list_of_unavailable_products_2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_products_2000 = pd.DataFrame(list_of_products_2000)\n",
    "\n",
    "list_of_products_2000.to_csv('list_of_products_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_2000 = pd.DataFrame(list_of_unavailable_products_2000)\n",
    "\n",
    "list_of_unavailable_products_2000.to_csv('list_of_unavailable_products_2000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Products 2000-4000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_products_4000 = []\n",
    "list_of_unavailable_products_4000 =[]\n",
    "\n",
    "\n",
    "#Instantiating the web driver(product 2000-2500)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through first 2000-2500 products in the asin list\n",
    "for i in unique_asin[2000:2500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_4000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_4000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 2500-3000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 2500-3000 products in the asin list\n",
    "for i in unique_asin[2500:3000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_4000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_4000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 3000-35000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 3000-3500 products in the asin list\n",
    "for i in unique_asin[3000:3500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_4000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_4000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 3500-4000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 3500-4000 products in the asin list\n",
    "for i in unique_asin[3500:4000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_4000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_4000.append(i)\n",
    "        print('not available',i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802\n",
      "1198\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_products_4000))\n",
    "print(len(list_of_unavailable_products_4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_products_4000 = pd.DataFrame(list_of_products_4000)\n",
    "\n",
    "list_of_products_4000.to_csv('list_of_products_4000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_4000 = pd.DataFrame(list_of_unavailable_products_4000)\n",
    "\n",
    "list_of_unavailable_products_4000.to_csv('list_of_unavailable_products_4000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Scrape Products 4000-6000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_products_6000 = []\n",
    "list_of_unavailable_products_6000 =[]\n",
    "\n",
    "#Instantiating the web driver(product 4000-4500)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through first 4000-4500 products in the asin list\n",
    "for i in unique_asin[4000:4500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_6000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_6000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 4500-5000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 4500-5000 products in the asin list\n",
    "for i in unique_asin[4500:5000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_6000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_6000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 5000-55000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 5000-5500 products in the asin list\n",
    "for i in unique_asin[5000:5500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_6000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_6000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 5500-6000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 5500-6000 products in the asin list\n",
    "for i in unique_asin[5500:6000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_6000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_6000.append(i)\n",
    "        print('not available',i)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781\n",
      "1219\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_products_6000))\n",
    "print(len(list_of_unavailable_products_6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_products_6000 = pd.DataFrame(list_of_products_6000)\n",
    "\n",
    "list_of_products_6000.to_csv('list_of_products_6000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_6000 = pd.DataFrame(list_of_unavailable_products_6000)\n",
    "\n",
    "list_of_unavailable_products_6000.to_csv('list_of_unavailable_products_6000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Products 6000-8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_products_8000 = []\n",
    "list_of_unavailable_products_8000 =[]\n",
    "\n",
    "\n",
    "#Instantiating the web driver(product 6000-6500)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through first 6000-6500 products in the asin list\n",
    "for i in unique_asin[6000:6500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_8000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_8000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 6500-7000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 6500-7000 products in the asin list\n",
    "for i in unique_asin[6500:7000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_8000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_8000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 7000-75000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 7000-7500 products in the asin list\n",
    "for i in unique_asin[7000:7500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_8000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_8000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 7500-8000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 7500-8000 products in the asin list\n",
    "for i in unique_asin[7500:8000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_8000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_8000.append(i)\n",
    "        print('not available',i)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654\n",
      "1346\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_products_8000))\n",
    "print(len(list_of_unavailable_products_8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_products_8000 = pd.DataFrame(list_of_products_8000)\n",
    "\n",
    "list_of_products_8000.to_csv('list_of_products_8000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_8000 = pd.DataFrame(list_of_unavailable_products_8000)\n",
    "\n",
    "list_of_unavailable_products_8000.to_csv('list_of_unavailable_products_8000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Products 8000-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_products_10000 = []\n",
    "list_of_unavailable_products_10000 =[]\n",
    "\n",
    "#Instantiating the web driver(product 8000-8500)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through first 8000-8500 products in the asin list\n",
    "for i in unique_asin[8000:8500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_10000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_10000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 8500-9000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 8500-9000 products in the asin list\n",
    "for i in unique_asin[8500:9000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_10000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_10000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 7000-75000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 9000-9500 products in the asin list\n",
    "for i in unique_asin[9000:9500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_10000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_10000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 9500-10000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 9500-10000 products in the asin list\n",
    "for i in unique_asin[9500:10000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_10000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_10000.append(i)\n",
    "        print('not available',i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567\n",
      "1433\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_products_10000))\n",
    "print(len(list_of_unavailable_products_10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_products_10000 = pd.DataFrame(list_of_products_10000)\n",
    "\n",
    "list_of_products_10000.to_csv('list_of_products_10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_10000 = pd.DataFrame(list_of_unavailable_products_10000)\n",
    "\n",
    "list_of_unavailable_products_10000.to_csv('list_of_unavailable_products_10000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Scraping products 10000-12101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_products_12000 = []\n",
    "list_of_unavailable_products_12000 =[]\n",
    "\n",
    "\n",
    "#Instantiating the web driver(product 10000-10500)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through first 10000-10500 products in the asin list\n",
    "for i in unique_asin[10000:10500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_12000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_12000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 10500-11000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 10500-11000 products in the asin list\n",
    "for i in unique_asin[10500:11000]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_12000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_12000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 11000-115000)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 11000-11500 products in the asin list\n",
    "for i in unique_asin[11000:11500]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_12000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_12000.append(i)\n",
    "        print('not available',i)\n",
    "        \n",
    "#Instantiating the web driver(product 11500-12101)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through 11500-12101 products in the asin list\n",
    "for i in unique_asin[11500:12101]:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_products_12000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_12000.append(i)\n",
    "        print('not available',i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509\n",
      "1592\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_products_12000))\n",
    "print(len(list_of_unavailable_products_12000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_products_12000 = pd.DataFrame(list_of_products_12000)\n",
    "\n",
    "list_of_products_12000.to_csv('list_of_products_12000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_12000 = pd.DataFrame(list_of_unavailable_products_12000)\n",
    "\n",
    "list_of_unavailable_products_12000.to_csv('list_of_unavailable_products_12000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the unavailable columns to check for anti robot block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list_of_unavailable_products_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_unavailable_products_2000=list_of_unavailable_products_2000.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['7806397051'],\n",
       " ['9759091062'],\n",
       " ['9790790961'],\n",
       " ['9790794231'],\n",
       " ['B00004U9UY'],\n",
       " ['B00004U9V2'],\n",
       " ['B000050B6U'],\n",
       " ['B000052WY7'],\n",
       " ['B000052WYL'],\n",
       " ['B000052XZP']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_unavailable_products_2000[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_unavailable_products_2000=oneDArray(list_of_unavailable_products_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7806397051',\n",
       " '9759091062',\n",
       " '9790790961',\n",
       " '9790794231',\n",
       " 'B00004U9UY',\n",
       " 'B00004U9V2',\n",
       " 'B000050B6U',\n",
       " 'B000052WY7',\n",
       " 'B000052WYL',\n",
       " 'B000052XZP']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_unavailable_products_2000[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_available_products_Rerun_2000 = []\n",
    "list_of_unavailable_products_Rerun_2000 =[]\n",
    "\n",
    "\n",
    "#Instantiating the web driver(product 10000-10500)\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through first 10000-10500 products in the asin list\n",
    "for i in list_of_unavailable_products_2000:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_available_products_Rerun_2000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_Rerun_2000.append(i)\n",
    "        print('not available',i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "1123\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_available_products_Rerun_2000))\n",
    "print(len(list_of_unavailable_products_Rerun_2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_available_products_Rerun_2000 = pd.DataFrame(list_of_available_products_Rerun_2000)\n",
    "\n",
    "list_of_available_products_Rerun_2000.to_csv('list_of_available_products_Rerun_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_Rerun_2000 = pd.DataFrame(list_of_unavailable_products_Rerun_2000)\n",
    "\n",
    "list_of_unavailable_products_Rerun_2000.to_csv('list_of_unavailable_products_Rerun_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list_of_available_products_Rerun_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B000PHRIQM',\n",
       " 'B000PHVNE0',\n",
       " 'B000PHZ26K',\n",
       " 'B000PHZE7M',\n",
       " 'B000PI3BS0',\n",
       " 'B000PI4J3G',\n",
       " 'B000PI57HS',\n",
       " 'B000PI9MO2',\n",
       " 'B000PK4XRQ',\n",
       " 'B000PKUYYW']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_unavailable_products_4000[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_unavailable_products_4000=list_of_unavailable_products_4000.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_unavailable_products_4000=oneDArray(list_of_unavailable_products_4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_available_products_Rerun_4000 = []\n",
    "list_of_unavailable_products_Rerun_4000 =[]\n",
    "\n",
    "\n",
    "#Instantiating the web driver\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through first 10000-10500 products in the asin list\n",
    "for i in list_of_unavailable_products_4000:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_available_products_Rerun_4000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_Rerun_4000.append(i)\n",
    "        print('not available',i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "1158\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_available_products_Rerun_4000))\n",
    "print(len(list_of_unavailable_products_Rerun_4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_available_products_Rerun_4000 = pd.DataFrame(list_of_available_products_Rerun_4000)\n",
    "\n",
    "list_of_available_products_Rerun_4000.to_csv('list_of_available_products_Rerun_4000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_Rerun_4000 = pd.DataFrame(list_of_unavailable_products_Rerun_4000)\n",
    "\n",
    "list_of_unavailable_products_Rerun_4000.to_csv('list_of_unavailable_products_Rerun_4000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_unavailable_products_6000=list_of_unavailable_products_6000.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_unavailable_products_6000=oneDArray(list_of_unavailable_products_6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_available_products_Rerun_6000 = []\n",
    "list_of_unavailable_products_Rerun_6000 =[]\n",
    "\n",
    "\n",
    "#Instantiating the web driver\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through first 10000-10500 products in the asin list\n",
    "for i in list_of_unavailable_products_6000:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_available_products_Rerun_6000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_Rerun_6000.append(i)\n",
    "        print('not available',i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "1207\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_available_products_Rerun_6000))\n",
    "print(len(list_of_unavailable_products_Rerun_6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_available_products_Rerun_6000 = pd.DataFrame(list_of_available_products_Rerun_6000)\n",
    "\n",
    "list_of_available_products_Rerun_6000.to_csv('list_of_available_products_Rerun_6000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_Rerun_6000 = pd.DataFrame(list_of_unavailable_products_Rerun_6000)\n",
    "\n",
    "list_of_unavailable_products_Rerun_6000.to_csv('list_of_unavailable_products_Rerun_6000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_unavailable_products_8000=list_of_unavailable_products_8000.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_unavailable_products_8000=oneDArray(list_of_unavailable_products_8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_available_products_Rerun_8000 = []\n",
    "list_of_unavailable_products_Rerun_8000 =[]\n",
    "\n",
    "\n",
    "#Instantiating the web driver\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/agarai/Desktop/Sweta-Data_Science/Python_Project1/chromedriver\")\n",
    "#driver = webdriver.Chrome(executable_path=\"/Users/swetaprabha/Desktop/Job_Preparation/RA_Recommender_System/chromedriver\")\n",
    "\n",
    "#Start looping through first 10000-10500 products in the asin list\n",
    "for i in list_of_unavailable_products_8000:  \n",
    "    \n",
    "    # Print the urls at the beginning of each loop \n",
    "    print(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    # Send webdriver to the url in the loop\n",
    "    driver.get(f'https://www.amazon.com/gp/product/{i}')\n",
    "    \n",
    "    \n",
    "    # Instantiate BeautifulSoup web scraper \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    \n",
    "    # Create an empty dictionary for each product with asin number i\n",
    "    product_info = {}\n",
    "    \n",
    "    #Try to locate tags to web scrape\n",
    "    try:\n",
    "        # Save product ID # so I can identify which product the scrape belongs to \n",
    "        product_info['asin'] = i\n",
    "\n",
    "        # Look for product title \n",
    "        product_info['name'] = soup.find('span', {'id':'productTitle'}).text.strip()\n",
    "        \n",
    "        # Look for product categories list \n",
    "        product_info['category'] = soup.find('ul', {'class': 'a-unordered-list a-horizontal a-size-small'}).findAll('li')[2].text.strip()\n",
    "         # Look for product description\n",
    "        product_info['description'] = soup.find('table', {'class':'a-normal a-spacing-micro'}).text.strip()\n",
    "        \n",
    "        #Look for Product Price\n",
    "        product_info['price'] = soup.find('span', {'id':'priceblock_ourprice'}).text.strip() \n",
    "        \n",
    "        #Look for Rating\n",
    "        product_info['rating']=soup.find('span', {'class':'a-icon-alt'}).text.strip()\n",
    "        \n",
    "        #Number of Ratings\n",
    "        product_info['No_of_Rating']=soup.find('span', {'id':'acrCustomerReviewText'}).text.strip()\n",
    "        \n",
    "        #Add to dictionary\n",
    "        print('adding to dictionary',i)\n",
    "        list_of_available_products_Rerun_8000.append(product_info)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        list_of_unavailable_products_Rerun_8000.append(i)\n",
    "        print('not available',i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_of_available_products_Rerun_8000))\n",
    "print(len(list_of_unavailable_products_Rerun_8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_available_products_Rerun_8000 = pd.DataFrame(list_of_available_products_Rerun_8000)\n",
    "\n",
    "list_of_available_products_Rerun_8000.to_csv('list_of_available_products_Rerun_8000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the list to pandas dataframe to write onto a csv file\n",
    "list_of_unavailable_products_Rerun_8000 = pd.DataFrame(list_of_unavailable_products_Rerun_8000)\n",
    "\n",
    "list_of_unavailable_products_Rerun_8000.to_csv('list_of_unavailable_products_Rerun_8000.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
